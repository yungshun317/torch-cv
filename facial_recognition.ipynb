{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "59a4b8dc-09fc-405f-8e55-23074a7954ca",
   "metadata": {},
   "source": [
    "# 1. Facial Detection with SSD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb07a465-fd95-4678-baf7-119df1297f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "\n",
    "path = 'data/yalefaces.zip'\n",
    "zip_object = zipfile.ZipFile(file = path, mode = 'r')\n",
    "zip_object.extractall('data')\n",
    "zip_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3765f2a7-a4d8-4931-bea9-664d5840d8ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['subject05.glasses.gif', 'subject08.wink.gif', 'subject05.centerlight.gif', 'subject01.sad.gif', 'subject06.noglasses.gif', 'subject08.centerlight.gif', 'subject11.sleepy.gif', 'subject04.happy.gif', 'subject11.wink.gif', 'subject08.noglasses.gif', 'subject05.wink.gif', 'subject03.sleepy.gif', 'subject15.centerlight.gif', 'subject11.surprised.gif', 'subject06.glasses.gif', 'subject05.happy.gif', 'subject10.normal.gif', 'subject03.centerlight.gif', 'subject04.normal.gif', 'subject09.noglasses.gif', 'subject08.sleepy.gif', 'subject06.rightlight.gif', 'subject14.noglasses.gif', 'subject02.noglasses.gif', 'subject06.centerlight.gif', 'subject09.centerlight.gif', 'subject04.noglasses.gif', 'subject12.glasses.gif', 'subject03.happy.gif', 'subject06.wink.gif', 'subject10.sleepy.gif', 'subject01.rightlight.gif', 'subject12.surprised.gif', 'subject03.surprised.gif', 'subject12.centerlight.gif', 'subject13.happy.gif', 'subject02.surprised.gif', 'subject09.leftlight.gif', 'subject10.leftlight.gif', 'subject14.sleepy.gif', 'subject14.rightlight.gif', 'subject15.normal.gif', 'subject14.happy.gif', 'subject14.wink.gif', 'subject02.rightlight.gif', 'subject09.happy.gif', 'subject12.happy.gif', 'subject12.sad.gif', 'subject11.centerlight.gif', 'subject01.normal.gif', 'subject04.sleepy.gif', 'subject13.noglasses.gif', 'subject14.leftlight.gif', 'subject07.rightlight.gif', 'subject14.surprised.gif', 'subject04.centerlight.gif', 'subject05.leftlight.gif', 'subject02.glasses.gif', 'subject08.leftlight.gif', 'subject06.sleepy.gif', 'subject10.noglasses.gif', 'subject15.glasses.gif', 'subject15.leftlight.gif', 'subject03.normal.gif', 'subject02.sleepy.gif', 'subject10.happy.gif', 'subject05.normal.gif', 'subject14.centerlight.gif', 'subject13.centerlight.gif', 'subject02.happy.gif', 'subject14.glasses.gif', 'subject07.centerlight.gif', 'subject07.sleepy.gif', 'subject01.surprised.gif', 'subject09.glasses.gif', 'subject05.sad.gif', 'subject04.sad.gif', 'subject04.wink.gif', 'subject02.sad.gif', 'subject12.leftlight.gif', 'subject11.leftlight.gif', 'subject01.noglasses.gif', 'subject09.surprised.gif', 'subject09.wink.gif', 'subject06.surprised.gif', 'subject04.rightlight.gif', 'subject13.wink.gif', 'subject05.noglasses.gif', 'subject13.normal.gif', 'subject07.noglasses.gif', 'subject04.glasses.gif', 'subject12.noglasses.gif', 'subject10.surprised.gif', 'subject07.normal.gif', 'subject11.sad.gif', 'subject15.surprised.gif', 'subject13.surprised.gif', 'subject05.rightlight.gif', 'subject07.glasses.gif', 'subject08.glasses.gif', 'subject10.glasses.gif', 'subject07.sad.gif', 'subject02.wink.gif', 'subject11.noglasses.gif', 'subject12.wink.gif', 'subject03.sad.gif', 'subject06.normal.gif', 'subject06.sad.gif', 'subject03.rightlight.gif', 'subject15.happy.gif', 'subject10.wink.gif', 'subject13.leftlight.gif', 'subject01.wink.gif', 'subject01.leftlight.gif', 'subject01.sleepy.gif', 'subject10.rightlight.gif', 'subject07.surprised.gif', 'subject08.sad.gif', 'subject03.wink.gif', 'subject13.glasses.gif', 'subject15.noglasses.gif', 'subject08.happy.gif', 'subject11.normal.gif', 'subject09.sleepy.gif', 'subject15.wink.gif', 'subject13.rightlight.gif', 'subject15.sleepy.gif', 'subject09.normal.gif', 'subject12.sleepy.gif', 'subject08.surprised.gif', 'subject02.normal.gif', 'subject11.rightlight.gif', 'subject07.wink.gif', 'subject01.glasses.gif', 'subject03.noglasses.gif']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "print(os.listdir('data/yalefaces/train'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1134a80f-d609-4ed4-9cfd-f589d60a98d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PIL.Image.Image"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "\n",
    "test_image = 'data/yalefaces/train/subject01.leftlight.gif'\n",
    "image = Image.open(test_image).convert('L')\n",
    "type(image)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fda59e15-ffe5-41c9-96d2-f4fc42bffb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(243, 320)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "# from google.colab.patches import cv2_imshow\n",
    "\n",
    "image_np = np.array(image, 'uint8')\n",
    "\n",
    "# Colab\n",
    "# cv2_imshow(image_np)\n",
    "\n",
    "# Jupyter lab\n",
    "cv2.imshow(\"Image\", image_np)\n",
    "# wait for ay key to exit window\n",
    "cv2.waitKey(0) \n",
    "# close all windows\n",
    "cv2.destroyAllWindows() \n",
    "\n",
    "print(image_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "784e1f79-6711-494e-9136-934137b599f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "network = cv2.dnn.readNetFromCaffe('model/deploy.prototxt.txt', 'model/res10_300x300_ssd_iter_140000.caffemodel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b18096-c09d-4f40-9297-8160ec4cf5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "image = cv2.cvtColor(image_np, cv2.COLOR_GRAY2BGR)\n",
    "(h, w) = image.shape[:2]\n",
    "blob = cv2.dnn.blobFromImage(cv2.resize(image, (100, 100)), 1.0, (100,100), (104.0, 117.0, 123.0))\n",
    "network.setInput(blob)\n",
    "detections = network.forward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b09e950-06ec-42bd-a91c-4a55285e434e",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf_min = 0.7\n",
    "image_cp = image.copy()\n",
    "for i in range(0, detections.shape[2]):\n",
    "  confidence = detections[0, 0, i, 2]\n",
    "  if confidence > conf_min:\n",
    "    bbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "    (start_x, start_y, end_x, end_y) = bbox.astype('int')\n",
    "    roi = image_cp[start_y:end_y, start_x:end_x]\n",
    "    text = \"{:.2f}%\".format(confidence * 100)\n",
    "    cv2.putText(image, text, (start_x, start_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0,255,0), 2)\n",
    "    cv2.rectangle(image, (start_x, start_y), (end_x, end_y), (0,255,0), 2)\n",
    "face = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "cv2_imshow(image)\n",
    "print(image.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894fba72-2f57-42d3-b48d-9155381f18b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cv2_imshow(face)\n",
    "print(face.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556a8444-7256-475a-91f7-c7ecad9424d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "face = cv2.resize(face, (60, 80))\n",
    "cv2_imshow(face)\n",
    "print(face.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6bef066-34a0-4dc9-8f41-58777f450664",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_face(network, image_path, conf_min = 0.7):\n",
    "  image = Image.open(image_path).convert('L')\n",
    "  image = np.array(image, 'uint8')\n",
    "  image = cv2.cvtColor(image, cv2.COLOR_GRAY2BGR)\n",
    "  (h, w) = image.shape[:2]\n",
    "  blob = cv2.dnn.blobFromImage(cv2.resize(image, (100, 100)), 1.0, (100, 100), (104.0, 117.0, 123.0))\n",
    "  network.setInput(blob)\n",
    "  detections = network.forward()\n",
    "\n",
    "  face = None\n",
    "  for i in range(0, detections.shape[2]):\n",
    "    confidence = detections[0, 0, i, 2]\n",
    "    if confidence > conf_min:\n",
    "      bbox = detections[0, 0, i, 3:7] * np.array([w, h, w, h])\n",
    "      (start_x, start_y, end_x, end_y) = bbox.astype(\"int\")\n",
    "\n",
    "      roi = image[start_y:end_y,start_x:end_x]\n",
    "      roi = cv2.resize(roi, (60, 80))\n",
    "      cv2.rectangle(image, (start_x, start_y), (end_x, end_y), (0, 255, 0), 2)\n",
    "      #text = \"{:.2f}%\".format(confidence * 100)\n",
    "      #cv2.putText(image, text, (start_x, start_y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 2)\n",
    "\n",
    "      face = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "  return face, image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb8f9fd-83e4-4e14-883e-497e8f1e720b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_image = '/content/yalefaces/train/subject02.noglasses.gif'\n",
    "face, image = detect_face(network, test_image)\n",
    "cv2_imshow(image)\n",
    "cv2_imshow(face)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "28471982-5ee7-42e2-a153-a4e905cc5ce9",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'detect_face' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m     cv2_imshow(face)\n\u001b[1;32m     17\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39marray(ids), faces\n\u001b[0;32m---> 19\u001b[0m ids, faces \u001b[38;5;241m=\u001b[39m \u001b[43mget_image_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[12], line 7\u001b[0m, in \u001b[0;36mget_image_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m ids \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m path \u001b[38;5;129;01min\u001b[39;00m paths:\n\u001b[0;32m----> 7\u001b[0m   face, image \u001b[38;5;241m=\u001b[39m \u001b[43mdetect_face\u001b[49m(network, path)\n\u001b[1;32m      8\u001b[0m   \u001b[38;5;66;03m#cv2_imshow(image)\u001b[39;00m\n\u001b[1;32m      9\u001b[0m   \u001b[38;5;66;03m#cv2_imshow(face)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m   \u001b[38;5;66;03m#print(path)\u001b[39;00m\n\u001b[1;32m     11\u001b[0m   \u001b[38;5;66;03m#print(os.path.split(path)[1])\u001b[39;00m\n\u001b[1;32m     12\u001b[0m   \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39msplit(path)[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubject\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'detect_face' is not defined"
     ]
    }
   ],
   "source": [
    "def get_image_data():\n",
    "  paths = [os.path.join('data/yalefaces/train', f) for f in os.listdir('data/yalefaces/train')]\n",
    "  #print(paths)\n",
    "  faces = []\n",
    "  ids = []\n",
    "  for path in paths:\n",
    "    face, image = detect_face(network, path)\n",
    "    #cv2_imshow(image)\n",
    "    #cv2_imshow(face)\n",
    "    #print(path)\n",
    "    #print(os.path.split(path)[1])\n",
    "    id = int(os.path.split(path)[1].split('.')[0].replace('subject', ''))\n",
    "    #print(id)\n",
    "    ids.append(id)\n",
    "    faces.append(face)\n",
    "    cv2_imshow(face)\n",
    "  return np.array(ids), faces\n",
    "\n",
    "ids, faces = get_image_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c454c9f-c315-48fe-ae99-904a5270c3e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1679eb71-a55b-4c88-96cc-3e73b439c543",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ids), len(faces)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d308f45-9f1d-46af-8cfc-0dca9ad93496",
   "metadata": {},
   "outputs": [],
   "source": [
    "faces[1], faces[1].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce484e66-6007-4ec1-910b-0088445cbf39",
   "metadata": {},
   "source": [
    "# 2. Facial Recognition with dlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8070dc63-6d15-4998-9761-3bf455a04282",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = 'data/celeb_dataset.zip'\n",
    "\n",
    "zip_object = zipfile.ZipFile(file=path, mode = 'r')\n",
    "zip_object.extractall('data')\n",
    "zip_object.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6c4d72c-c898-402e-89c0-6bf3748972bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "def load_training(path_dataset, max_width=400):\n",
    "  index = {}\n",
    "  idx = 0\n",
    "  face_descriptors = None\n",
    "\n",
    "  subdirs = [os.path.join(path_dataset, f) for f in os.listdir(path_dataset)]\n",
    "\n",
    "  for subdir in subdirs:\n",
    "    name = subdir.split(os.path.sep)[-1]\n",
    "    print(name)\n",
    "    images_list = [os.path.join(subdir, f) for f in os.listdir(subdir)]\n",
    "    for image_path in images_list:\n",
    "      image = Image.open(image_path).convert('RGB')\n",
    "      image_np = np.array(image, 'uint8')\n",
    "\n",
    "      if (image_np.shape[1] > max_width):\n",
    "        image_np = imutils.resize(image_np, width=max_width)\n",
    "\n",
    "      face_detection = face_detector(image_np, 1)\n",
    "      for face in face_detection:\n",
    "        image_np, face_descriptors = extract_descriptor(face, image_np, face_descriptors)\n",
    "\n",
    "        index[idx] = image_path\n",
    "        idx += 1\n",
    "      cv2_imshow(cv2.cvtColor(image_np, cv2.COLOR_RGB2BGR))\n",
    "\n",
    "  return face_descriptors, index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb69b9e6-d254-4848-a71f-a8098fc665f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "face_descriptors, index = load_training('data/celeb_dataset/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e94c450-48b2-4740-9ce3-314a3279f548",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(\"dataset_face_descriptors.npy\", face_descriptors)\n",
    "with open(\"dataset_images_index.pickle\", \"wb\") as f:\n",
    "  pickle.dump(index, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8927d41c-9a57-48fd-8b49-cdb904deebf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(face_descriptors), len(index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "736c847e-8f2e-49bf-92ca-7b3834b2d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "index"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
